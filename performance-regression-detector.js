/**
 * Performance Regression Detector - Automated performance regression detection
 * Stores baselines and detects performance regressions over time
 * 
 * Enhanced Claude Development Protocol v1.4 - Phase 3: Refinement
 */

const redis = require('redis');
const { Client } = require('pg');
const PerformanceTestSuite = require('./performance-test-suite');
const CVLatencyBenchmark = require('./cv-latency-benchmark');

class PerformanceRegressionDetector {
    constructor() {
        this.redisClient = null;
        this.pgClient = null;
        this.testSuite = null;
        this.benchmark = null;
        
        // Regression detection configuration
        this.config = {
            // Baseline management
            baselineRetentionDays: 30,
            minimumBaselines: 5,
            baselineUpdateThreshold: 0.1, // 10% improvement to update baseline
            
            // Regression detection thresholds
            regressionThresholds: {\n                latency: {\n                    warning: 0.15,    // 15% degradation warning\n                    critical: 0.30,   // 30% degradation critical\n                    absolute: 15      // 15ms absolute threshold\n                },\n                throughput: {\n                    warning: 0.10,    // 10% reduction warning\n                    critical: 0.25,   // 25% reduction critical\n                    absolute: 50      // 50 req/sec minimum\n                },\n                memory: {\n                    warning: 0.20,    // 20% increase warning\n                    critical: 0.50,   // 50% increase critical\n                    absolute: 500     // 500MB absolute limit\n                },\n                errorRate: {\n                    warning: 0.02,    // 2% error rate warning\n                    critical: 0.05,   // 5% error rate critical\n                    absolute: 0.01    // 1% baseline threshold\n                }\n            },\n            \n            // Statistical analysis\n            statisticalConfig: {\n                confidenceLevel: 0.95,\n                minimumSamples: 10,\n                outlierThreshold: 2.0, // 2 standard deviations\n                trendAnalysisWindow: 20 // Last 20 measurements for trend\n            },\n            \n            // Automated testing\n            automatedTesting: {\n                enabled: process.env.PERF_REGRESSION_AUTO === 'true',\n                intervalMs: 3600000, // 1 hour\n                quickTestIntervalMs: 300000, // 5 minutes\n                baselineUpdateIntervalMs: 86400000 // 24 hours\n            }\n        };\n        \n        // State tracking\n        this.state = {\n            currentBaselines: new Map(),\n            regressionHistory: [],\n            activeAlerts: new Set(),\n            lastBaselineUpdate: 0,\n            performanceHistory: [],\n            trendAnalysis: null\n        };\n        \n        console.log('📈 Performance Regression Detector initialized');\n    }\n    \n    async init() {\n        await this.setupDatabase();\n        await this.setupPerformanceTools();\n        await this.loadExistingBaselines();\n        \n        if (this.config.automatedTesting.enabled) {\n            this.startAutomatedTesting();\n        }\n        \n        console.log('🚀 Performance Regression Detector ready');\n    }\n    \n    async setupDatabase() {\n        // Redis connection\n        this.redisClient = redis.createClient({\n            url: process.env.REDIS_URL || 'redis://localhost:6379'\n        });\n        await this.redisClient.connect();\n        \n        // PostgreSQL connection\n        try {\n            this.pgClient = new Client({\n                connectionString: process.env.DATABASE_URL || 'postgresql://localhost:5432/twitch_cv'\n            });\n            await this.pgClient.connect();\n            \n            // Create performance tracking tables\n            await this.createPerformanceTables();\n            \n            console.log('✅ Regression detector connected to databases');\n        } catch (err) {\n            console.log('⚠️ PostgreSQL unavailable for regression detection');\n            this.pgClient = null;\n        }\n    }\n    \n    async createPerformanceTables() {\n        if (!this.pgClient) return;\n        \n        const createTablesSQL = `\n            CREATE TABLE IF NOT EXISTS performance_baselines (\n                id SERIAL PRIMARY KEY,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                metric_type VARCHAR(50) NOT NULL,\n                test_scenario VARCHAR(100) NOT NULL,\n                baseline_value FLOAT NOT NULL,\n                statistical_data JSONB NOT NULL,\n                measurement_count INTEGER DEFAULT 1,\n                confidence_interval JSONB,\n                metadata JSONB\n            );\n            \n            CREATE TABLE IF NOT EXISTS performance_measurements (\n                id SERIAL PRIMARY KEY,\n                measured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                metric_type VARCHAR(50) NOT NULL,\n                test_scenario VARCHAR(100) NOT NULL,\n                measured_value FLOAT NOT NULL,\n                test_duration INTEGER,\n                test_parameters JSONB,\n                environment_data JSONB,\n                regression_detected BOOLEAN DEFAULT FALSE,\n                regression_severity VARCHAR(20)\n            );\n            \n            CREATE TABLE IF NOT EXISTS performance_regressions (\n                id SERIAL PRIMARY KEY,\n                detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                metric_type VARCHAR(50) NOT NULL,\n                test_scenario VARCHAR(100) NOT NULL,\n                baseline_value FLOAT NOT NULL,\n                measured_value FLOAT NOT NULL,\n                regression_percent FLOAT NOT NULL,\n                severity VARCHAR(20) NOT NULL,\n                status VARCHAR(20) DEFAULT 'active',\n                resolved_at TIMESTAMP,\n                analysis_data JSONB,\n                alert_data JSONB\n            );\n            \n            CREATE INDEX IF NOT EXISTS idx_baselines_type_scenario ON performance_baselines(metric_type, test_scenario);\n            CREATE INDEX IF NOT EXISTS idx_measurements_time ON performance_measurements(measured_at);\n            CREATE INDEX IF NOT EXISTS idx_regressions_detected ON performance_regressions(detected_at, status);\n        `;\n        \n        await this.pgClient.query(createTablesSQL);\n        console.log('📊 Performance tracking tables initialized');\n    }\n    \n    async setupPerformanceTools() {\n        // Initialize test suite and benchmark\n        this.testSuite = new PerformanceTestSuite();\n        await this.testSuite.init();\n        \n        this.benchmark = new CVLatencyBenchmark();\n        await this.benchmark.init();\n    }\n    \n    async loadExistingBaselines() {\n        console.log('📊 Loading existing performance baselines...');\n        \n        if (!this.pgClient) {\n            // Load from Redis if PostgreSQL unavailable\n            const redisBaselines = await this.redisClient.hGetAll('perf:baselines');\n            Object.entries(redisBaselines).forEach(([key, value]) => {\n                this.state.currentBaselines.set(key, JSON.parse(value));\n            });\n            return;\n        }\n        \n        const query = `\n            SELECT metric_type, test_scenario, baseline_value, statistical_data, \n                   confidence_interval, measurement_count, updated_at\n            FROM performance_baselines\n            WHERE updated_at > NOW() - INTERVAL '${this.config.baselineRetentionDays} days'\n            ORDER BY updated_at DESC\n        `;\n        \n        const result = await this.pgClient.query(query);\n        \n        result.rows.forEach(row => {\n            const key = `${row.metric_type}_${row.test_scenario}`;\n            this.state.currentBaselines.set(key, {\n                metricType: row.metric_type,\n                testScenario: row.test_scenario,\n                baselineValue: row.baseline_value,\n                statisticalData: row.statistical_data,\n                confidenceInterval: row.confidence_interval,\n                measurementCount: row.measurement_count,\n                updatedAt: row.updated_at\n            });\n        });\n        \n        console.log(`📊 Loaded ${this.state.currentBaselines.size} performance baselines`);\n    }\n    \n    // ===== Baseline Management =====\n    \n    async establishBaseline(metricType, testScenario, measurements) {\n        console.log(`📊 Establishing baseline for ${metricType} - ${testScenario}`);\n        \n        const statisticalData = this.calculateStatistics(measurements);\n        const confidenceInterval = this.calculateConfidenceInterval(measurements, this.config.statisticalConfig.confidenceLevel);\n        \n        const baseline = {\n            metricType,\n            testScenario,\n            baselineValue: statisticalData.mean,\n            statisticalData,\n            confidenceInterval,\n            measurementCount: measurements.length,\n            createdAt: Date.now(),\n            updatedAt: Date.now()\n        };\n        \n        // Store baseline\n        await this.storeBaseline(baseline);\n        \n        // Update in-memory state\n        const key = `${metricType}_${testScenario}`;\n        this.state.currentBaselines.set(key, baseline);\n        \n        console.log(`✅ Baseline established: ${metricType} = ${baseline.baselineValue.toFixed(2)}`);\n        return baseline;\n    }\n    \n    async updateBaseline(metricType, testScenario, newMeasurements) {\n        const key = `${metricType}_${testScenario}`;\n        const existingBaseline = this.state.currentBaselines.get(key);\n        \n        if (!existingBaseline) {\n            return await this.establishBaseline(metricType, testScenario, newMeasurements);\n        }\n        \n        // Check if update is warranted (significant improvement)\n        const newStats = this.calculateStatistics(newMeasurements);\n        const improvementPercent = (existingBaseline.baselineValue - newStats.mean) / existingBaseline.baselineValue;\n        \n        if (improvementPercent > this.config.baselineUpdateThreshold) {\n            console.log(`📈 Updating baseline due to ${(improvementPercent * 100).toFixed(1)}% improvement`);\n            \n            const updatedBaseline = {\n                ...existingBaseline,\n                baselineValue: newStats.mean,\n                statisticalData: newStats,\n                confidenceInterval: this.calculateConfidenceInterval(newMeasurements, this.config.statisticalConfig.confidenceLevel),\n                measurementCount: newMeasurements.length,\n                updatedAt: Date.now()\n            };\n            \n            await this.storeBaseline(updatedBaseline);\n            this.state.currentBaselines.set(key, updatedBaseline);\n            \n            return updatedBaseline;\n        }\n        \n        return existingBaseline;\n    }\n    \n    async storeBaseline(baseline) {\n        // Store in Redis\n        const key = `${baseline.metricType}_${baseline.testScenario}`;\n        await this.redisClient.hSet('perf:baselines', key, JSON.stringify(baseline));\n        \n        // Store in PostgreSQL\n        if (this.pgClient) {\n            const query = `\n                INSERT INTO performance_baselines \n                (metric_type, test_scenario, baseline_value, statistical_data, confidence_interval, measurement_count, metadata)\n                VALUES ($1, $2, $3, $4, $5, $6, $7)\n                ON CONFLICT (metric_type, test_scenario) \n                DO UPDATE SET \n                    baseline_value = EXCLUDED.baseline_value,\n                    statistical_data = EXCLUDED.statistical_data,\n                    confidence_interval = EXCLUDED.confidence_interval,\n                    measurement_count = EXCLUDED.measurement_count,\n                    updated_at = CURRENT_TIMESTAMP\n            `;\n            \n            await this.pgClient.query(query, [\n                baseline.metricType,\n                baseline.testScenario,\n                baseline.baselineValue,\n                JSON.stringify(baseline.statisticalData),\n                JSON.stringify(baseline.confidenceInterval),\n                baseline.measurementCount,\n                JSON.stringify({ createdAt: baseline.createdAt })\n            ]);\n        }\n    }\n    \n    // ===== Regression Detection =====\n    \n    async detectRegressions(metricType, testScenario, newMeasurement, testMetadata = {}) {\n        const key = `${metricType}_${testScenario}`;\n        const baseline = this.state.currentBaselines.get(key);\n        \n        if (!baseline) {\n            console.log(`⚠️ No baseline found for ${metricType} - ${testScenario}`);\n            return null;\n        }\n        \n        const regressionAnalysis = this.analyzeRegression(baseline, newMeasurement, metricType);\n        \n        // Store measurement\n        await this.storeMeasurement({\n            metricType,\n            testScenario,\n            measuredValue: newMeasurement,\n            testMetadata,\n            regressionDetected: regressionAnalysis.isRegression,\n            regressionSeverity: regressionAnalysis.severity\n        });\n        \n        // If regression detected, create regression record and alert\n        if (regressionAnalysis.isRegression) {\n            await this.handleRegression(metricType, testScenario, baseline, newMeasurement, regressionAnalysis);\n        }\n        \n        return regressionAnalysis;\n    }\n    \n    analyzeRegression(baseline, measurement, metricType) {\n        const baselineValue = baseline.baselineValue;\n        const thresholds = this.config.regressionThresholds[metricType];\n        \n        if (!thresholds) {\n            return { isRegression: false, reason: 'no_thresholds' };\n        }\n        \n        // Calculate percentage change\n        let percentChange;\n        let isNegativeRegression;\n        \n        switch (metricType) {\n            case 'latency':\n            case 'memory':\n            case 'errorRate':\n                // Higher values are worse\n                percentChange = (measurement - baselineValue) / baselineValue;\n                isNegativeRegression = percentChange > 0;\n                break;\n            case 'throughput':\n                // Lower values are worse\n                percentChange = (baselineValue - measurement) / baselineValue;\n                isNegativeRegression = percentChange > 0;\n                break;\n            default:\n                return { isRegression: false, reason: 'unknown_metric_type' };\n        }\n        \n        // Check absolute thresholds\n        const absoluteThreshold = thresholds.absolute;\n        let absoluteViolation = false;\n        \n        switch (metricType) {\n            case 'latency':\n                absoluteViolation = measurement > absoluteThreshold;\n                break;\n            case 'memory':\n                absoluteViolation = measurement > absoluteThreshold;\n                break;\n            case 'throughput':\n                absoluteViolation = measurement < absoluteThreshold;\n                break;\n            case 'errorRate':\n                absoluteViolation = measurement > absoluteThreshold;\n                break;\n        }\n        \n        // Determine severity\n        let severity = 'none';\n        let isRegression = false;\n        \n        if (absoluteViolation || (isNegativeRegression && percentChange >= thresholds.critical)) {\n            severity = 'critical';\n            isRegression = true;\n        } else if (isNegativeRegression && percentChange >= thresholds.warning) {\n            severity = 'warning';\n            isRegression = true;\n        }\n        \n        // Statistical significance check\n        const statisticalSignificance = this.checkStatisticalSignificance(baseline, measurement);\n        \n        return {\n            isRegression,\n            severity,\n            percentChange: Math.abs(percentChange),\n            absoluteViolation,\n            baselineValue,\n            measuredValue: measurement,\n            statisticalSignificance,\n            confidenceLevel: this.config.statisticalConfig.confidenceLevel\n        };\n    }\n    \n    checkStatisticalSignificance(baseline, measurement) {\n        const stats = baseline.statisticalData;\n        const confidence = baseline.confidenceInterval;\n        \n        // Check if measurement falls outside confidence interval\n        const outsideConfidenceInterval = measurement < confidence.lower || measurement > confidence.upper;\n        \n        // Check if measurement is beyond outlier threshold\n        const standardDeviations = Math.abs(measurement - stats.mean) / stats.standardDeviation;\n        const isOutlier = standardDeviations > this.config.statisticalConfig.outlierThreshold;\n        \n        return {\n            outsideConfidenceInterval,\n            isOutlier,\n            standardDeviations,\n            confidenceInterval: confidence\n        };\n    }\n    \n    async handleRegression(metricType, testScenario, baseline, measurement, analysis) {\n        console.log(`🚨 Regression detected: ${metricType} - ${analysis.severity}`);\n        \n        // Create regression record\n        const regression = {\n            detectedAt: Date.now(),\n            metricType,\n            testScenario,\n            baselineValue: baseline.baselineValue,\n            measuredValue: measurement,\n            regressionPercent: analysis.percentChange * 100,\n            severity: analysis.severity,\n            status: 'active',\n            analysisData: analysis\n        };\n        \n        // Store regression\n        await this.storeRegression(regression);\n        \n        // Add to regression history\n        this.state.regressionHistory.push(regression);\n        \n        // Trigger alerts\n        await this.triggerRegressionAlert(regression);\n        \n        // Update active alerts\n        const alertKey = `${metricType}_${testScenario}_${analysis.severity}`;\n        this.state.activeAlerts.add(alertKey);\n        \n        return regression;\n    }\n    \n    async storeMeasurement(measurement) {\n        // Store in Redis\n        const measurementKey = `measurement_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n        await this.redisClient.hSet('perf:measurements', measurementKey, JSON.stringify({\n            ...measurement,\n            timestamp: Date.now()\n        }));\n        \n        // Store in PostgreSQL\n        if (this.pgClient) {\n            const query = `\n                INSERT INTO performance_measurements \n                (metric_type, test_scenario, measured_value, test_parameters, regression_detected, regression_severity)\n                VALUES ($1, $2, $3, $4, $5, $6)\n            `;\n            \n            await this.pgClient.query(query, [\n                measurement.metricType,\n                measurement.testScenario,\n                measurement.measuredValue,\n                JSON.stringify(measurement.testMetadata),\n                measurement.regressionDetected,\n                measurement.regressionSeverity\n            ]);\n        }\n    }\n    \n    async storeRegression(regression) {\n        // Store in Redis\n        const regressionKey = `regression_${Date.now()}_${regression.metricType}_${regression.testScenario}`;\n        await this.redisClient.hSet('perf:regressions', regressionKey, JSON.stringify(regression));\n        \n        // Store in PostgreSQL\n        if (this.pgClient) {\n            const query = `\n                INSERT INTO performance_regressions \n                (metric_type, test_scenario, baseline_value, measured_value, regression_percent, severity, analysis_data)\n                VALUES ($1, $2, $3, $4, $5, $6, $7)\n            `;\n            \n            await this.pgClient.query(query, [\n                regression.metricType,\n                regression.testScenario,\n                regression.baselineValue,\n                regression.measuredValue,\n                regression.regressionPercent,\n                regression.severity,\n                JSON.stringify(regression.analysisData)\n            ]);\n        }\n    }\n    \n    async triggerRegressionAlert(regression) {\n        const alert = {\n            type: 'performance_regression',\n            severity: regression.severity,\n            metric: regression.metricType,\n            scenario: regression.testScenario,\n            regression: regression.regressionPercent,\n            baseline: regression.baselineValue,\n            measured: regression.measuredValue,\n            timestamp: Date.now()\n        };\n        \n        // Publish to existing alert system\n        await this.redisClient.publish('cv:system:broadcast', JSON.stringify({\n            type: 'performance_regression_alert',\n            data: alert,\n            timestamp: Date.now()\n        }));\n        \n        console.log(`🚨 Regression alert triggered: ${regression.metricType} degraded by ${regression.regressionPercent.toFixed(1)}%`);\n    }\n    \n    // ===== Automated Testing and Monitoring =====\n    \n    startAutomatedTesting() {\n        console.log('🤖 Starting automated regression testing...');\n        \n        // Quick performance checks\n        setInterval(async () => {\n            await this.runQuickRegressionCheck();\n        }, this.config.automatedTesting.quickTestIntervalMs);\n        \n        // Comprehensive performance tests\n        setInterval(async () => {\n            await this.runComprehensiveRegressionTest();\n        }, this.config.automatedTesting.intervalMs);\n        \n        // Baseline updates\n        setInterval(async () => {\n            await this.updateBaselinesFromHistory();\n        }, this.config.automatedTesting.baselineUpdateIntervalMs);\n    }\n    \n    async runQuickRegressionCheck() {\n        console.log('⚡ Running quick regression check...');\n        \n        try {\n            // Run quick latency benchmark\n            const latencyResults = await this.benchmark.runLatencyBenchmark();\n            \n            // Check for regressions\n            await this.detectRegressions('latency', 'quick_check', latencyResults.latencyStatistics.mean, {\n                testType: 'quick_regression_check',\n                iterations: latencyResults.summary.successfulIterations\n            });\n            \n        } catch (err) {\n            console.error('Quick regression check error:', err.message);\n        }\n    }\n    \n    async runComprehensiveRegressionTest() {\n        console.log('🧪 Running comprehensive regression test...');\n        \n        try {\n            // Run sustained load test\n            const sustainedResults = await this.testSuite.runTest('sustained', {\n                duration: 180000, // 3 minutes\n                users: 100\n            });\n            \n            // Check multiple metrics for regressions\n            const analysis = sustainedResults.analysis;\n            \n            await this.detectRegressions('latency', 'sustained_load', analysis.latencyAnalysis.mean);\n            await this.detectRegressions('throughput', 'sustained_load', analysis.throughputAnalysis.overallThroughput);\n            \n            if (analysis.resourceAnalysis && analysis.resourceAnalysis.memory) {\n                await this.detectRegressions('memory', 'sustained_load', analysis.resourceAnalysis.memory.maxMB);\n            }\n            \n        } catch (err) {\n            console.error('Comprehensive regression test error:', err.message);\n        }\n    }\n    \n    async updateBaselinesFromHistory() {\n        console.log('📊 Updating baselines from historical data...');\n        \n        if (!this.pgClient) return;\n        \n        // Get recent measurements for baseline updates\n        const query = `\n            SELECT metric_type, test_scenario, measured_value\n            FROM performance_measurements\n            WHERE measured_at > NOW() - INTERVAL '7 days'\n            AND regression_detected = false\n            ORDER BY measured_at DESC\n        `;\n        \n        const result = await this.pgClient.query(query);\n        \n        // Group measurements by metric and scenario\n        const grouped = {};\n        result.rows.forEach(row => {\n            const key = `${row.metric_type}_${row.test_scenario}`;\n            if (!grouped[key]) {\n                grouped[key] = {\n                    metricType: row.metric_type,\n                    testScenario: row.test_scenario,\n                    measurements: []\n                };\n            }\n            grouped[key].measurements.push(row.measured_value);\n        });\n        \n        // Update baselines for metrics with sufficient data\n        for (const [key, data] of Object.entries(grouped)) {\n            if (data.measurements.length >= this.config.statisticalConfig.minimumSamples) {\n                await this.updateBaseline(data.metricType, data.testScenario, data.measurements);\n            }\n        }\n    }\n    \n    // ===== Statistical Analysis =====\n    \n    calculateStatistics(measurements) {\n        const sorted = [...measurements].sort((a, b) => a - b);\n        const n = sorted.length;\n        \n        const mean = sorted.reduce((sum, val) => sum + val, 0) / n;\n        const variance = sorted.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / n;\n        const standardDeviation = Math.sqrt(variance);\n        \n        return {\n            count: n,\n            mean,\n            median: sorted[Math.floor(n / 2)],\n            min: sorted[0],\n            max: sorted[n - 1],\n            variance,\n            standardDeviation,\n            percentiles: {\n                p25: sorted[Math.floor(n * 0.25)],\n                p75: sorted[Math.floor(n * 0.75)],\n                p90: sorted[Math.floor(n * 0.90)],\n                p95: sorted[Math.floor(n * 0.95)],\n                p99: sorted[Math.floor(n * 0.99)]\n            }\n        };\n    }\n    \n    calculateConfidenceInterval(measurements, confidenceLevel) {\n        const stats = this.calculateStatistics(measurements);\n        const tValue = this.getTValue(confidenceLevel, stats.count - 1);\n        const marginOfError = tValue * (stats.standardDeviation / Math.sqrt(stats.count));\n        \n        return {\n            confidenceLevel,\n            lower: stats.mean - marginOfError,\n            upper: stats.mean + marginOfError,\n            marginOfError\n        };\n    }\n    \n    getTValue(confidenceLevel, degreesOfFreedom) {\n        // Simplified t-value lookup for common confidence levels\n        const tTable = {\n            0.90: { 5: 2.015, 10: 1.812, 20: 1.725, 30: 1.697, 100: 1.660 },\n            0.95: { 5: 2.571, 10: 2.228, 20: 2.086, 30: 2.042, 100: 1.984 },\n            0.99: { 5: 4.032, 10: 3.169, 20: 2.845, 30: 2.750, 100: 2.626 }\n        };\n        \n        const levelTable = tTable[confidenceLevel] || tTable[0.95];\n        \n        // Find closest degrees of freedom\n        const dfs = Object.keys(levelTable).map(Number).sort((a, b) => a - b);\n        const closestDf = dfs.reduce((prev, curr) => \n            Math.abs(curr - degreesOfFreedom) < Math.abs(prev - degreesOfFreedom) ? curr : prev\n        );\n        \n        return levelTable[closestDf];\n    }\n    \n    // ===== Reporting and Analysis =====\n    \n    async generateRegressionReport() {\n        console.log('📊 Generating regression analysis report...');\n        \n        const report = {\n            timestamp: Date.now(),\n            summary: await this.getRegressionSummary(),\n            activeRegressions: await this.getActiveRegressions(),\n            baselines: this.getBaselineSummary(),\n            trends: await this.analyzeTrends(),\n            recommendations: await this.generateRegressionRecommendations()\n        };\n        \n        // Store report\n        await this.redisClient.hSet('perf:regression:reports', Date.now().toString(), JSON.stringify(report));\n        \n        return report;\n    }\n    \n    async getRegressionSummary() {\n        if (!this.pgClient) {\n            return {\n                totalRegressions: this.state.regressionHistory.length,\n                activeRegressions: this.state.activeAlerts.size\n            };\n        }\n        \n        const query = `\n            SELECT severity, COUNT(*) as count\n            FROM performance_regressions\n            WHERE detected_at > NOW() - INTERVAL '7 days'\n            GROUP BY severity\n        `;\n        \n        const result = await this.pgClient.query(query);\n        \n        const summary = {\n            totalRegressions: result.rows.reduce((sum, row) => sum + parseInt(row.count), 0),\n            bySeverity: {},\n            activeAlerts: this.state.activeAlerts.size\n        };\n        \n        result.rows.forEach(row => {\n            summary.bySeverity[row.severity] = parseInt(row.count);\n        });\n        \n        return summary;\n    }\n    \n    async getActiveRegressions() {\n        if (!this.pgClient) {\n            return Array.from(this.state.activeAlerts);\n        }\n        \n        const query = `\n            SELECT metric_type, test_scenario, severity, regression_percent, detected_at\n            FROM performance_regressions\n            WHERE status = 'active'\n            ORDER BY detected_at DESC\n            LIMIT 20\n        `;\n        \n        const result = await this.pgClient.query(query);\n        return result.rows;\n    }\n    \n    getBaselineSummary() {\n        const baselines = Array.from(this.state.currentBaselines.entries()).map(([key, baseline]) => ({\n            key,\n            metricType: baseline.metricType,\n            testScenario: baseline.testScenario,\n            baselineValue: baseline.baselineValue,\n            measurementCount: baseline.measurementCount,\n            updatedAt: baseline.updatedAt\n        }));\n        \n        return {\n            totalBaselines: baselines.length,\n            baselines: baselines.slice(0, 10), // Top 10 most recent\n            oldestBaseline: Math.min(...baselines.map(b => b.updatedAt)),\n            newestBaseline: Math.max(...baselines.map(b => b.updatedAt))\n        };\n    }\n    \n    sleep(ms) {\n        return new Promise(resolve => setTimeout(resolve, ms));\n    }\n    \n    async cleanup() {\n        if (this.testSuite) {\n            await this.testSuite.cleanup();\n        }\n        \n        if (this.benchmark) {\n            await this.benchmark.cleanup();\n        }\n        \n        if (this.redisClient) {\n            await this.redisClient.quit();\n        }\n        \n        if (this.pgClient) {\n            await this.pgClient.end();\n        }\n        \n        console.log('🧹 Performance Regression Detector cleanup completed');\n    }\n}\n\nmodule.exports = PerformanceRegressionDetector;\n\n// CLI interface\nif (require.main === module) {\n    const detector = new PerformanceRegressionDetector();\n    \n    async function runDetectorCLI() {\n        try {\n            await detector.init();\n            \n            const command = process.argv[2] || 'monitor';\n            \n            switch (command) {\n                case 'baseline':\n                    console.log('📊 Establishing new baselines...');\n                    // This would run comprehensive tests to establish baselines\n                    break;\n                    \n                case 'check':\n                    console.log('🔍 Running regression check...');\n                    await detector.runComprehensiveRegressionTest();\n                    break;\n                    \n                case 'report':\n                    console.log('📊 Generating regression report...');\n                    const report = await detector.generateRegressionReport();\n                    console.log('📊 Regression Report:', JSON.stringify(report, null, 2));\n                    break;\n                    \n                case 'monitor':\n                default:\n                    console.log('🔍 Starting regression monitoring...');\n                    // Keep running for automated monitoring\n                    process.on('SIGINT', async () => {\n                        console.log('\\n🛑 Shutting down regression monitoring...');\n                        await detector.cleanup();\n                        process.exit(0);\n                    });\n                    break;\n            }\n            \n        } catch (err) {\n            console.error('❌ Regression detection failed:', err);\n            await detector.cleanup();\n            process.exit(1);\n        }\n    }\n    \n    runDetectorCLI();\n}

<system-reminder>
Whenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.
</system-reminder>
</output>
</result>